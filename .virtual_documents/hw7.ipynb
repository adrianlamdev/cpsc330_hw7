# Initialize Otter
import otter
grader = otter.Notebook("hw7.ipynb")








import os

%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline, make_pipeline














import gensim
import gensim.downloader

print(list(gensim.downloader.info()["models"].keys()))


# This will take a while to run when you run it for the first time.
import gensim.downloader as api

glove_wiki_vectors = api.load("glove-wiki-gigaword-100")


len(glove_wiki_vectors)

















words_to_explore = ["python", "rock", "science", "dream"]

for word in words_to_explore:
    print(f"Similar words to {word}:")
    df = pd.DataFrame(glove_wiki_vectors.most_similar(word), columns=["Word", "Similarity"])
    display(df)








word_pairs = [
    ("coast", "shore"),
    ("clothes", "closet"),
    ("old", "new"),
    ("smart", "intelligent"),
    ("dog", "cat"),
    ("tree", "lawyer"),
]





similarities = []

for word1, word2 in word_pairs:
    similarity_score = glove_wiki_vectors.similarity(word1, word2)
    similarities.append((word1, word2, similarity_score))

similarities_df = pd.DataFrame(similarities, columns=["Word 1", "Word 2", "Similarity"])
display(similarities_df)








test_words = [
    "covididiot",
    "fomo",
    "frenemies",
    "anthropause",
    "photobomb",
    "selfie",
    "pxg",  # Abbreviation for pseudoexfoliative glaucoma
    "pacg",  # Abbreviation for primary angle closure glaucoma
    "cct",  # Abbreviation for central corneal thickness
    "escc",  # Abbreviation for esophageal squamous cell carcinoma
]





results = []

for word in test_words:
    if word in glove_wiki_vectors:
        results.append({
            'Word': word,
            'Exists in glove_wiki_vectors': True,
        })
    else:
        results.append({
            'Word': word,
            'Exists in glove_wiki_vectors': False,
        })

results_df = pd.DataFrame(results)
display(results_df)










def analogy(word1, word2, word3, model=glove_wiki_vectors):
    """
    Returns analogy word using the given model.

    Parameters
    --------------
    word1 : (str)
        word1 in the analogy relation
    word2 : (str)
        word2 in the analogy relation
    word3 : (str)
        word3 in the analogy relation
    model :
        word embedding model

    Returns
    ---------------
        pd.dataframe
    """
    print("%s : %s :: %s : ?" % (word1, word2, word3))
    sim_words = model.most_similar(positive=[word3, word2], negative=[word1])
    return pd.DataFrame(sim_words, columns=["Analogy word", "Score"])





analogy("man", "doctor", "woman")


glove_wiki_vectors.similarity("aboriginal", "success")


glove_wiki_vectors.similarity("white", "success")





analogy("man", "heterosexual", "woman")


analogy("white", "professional", "black")


analogy("rich", "educated", "poor")


analogy("young", "energetic", "old")




















from sklearn.datasets import fetch_20newsgroups


cats = [
    "rec.sport.hockey",
    "rec.sport.baseball",
    "soc.religion.christian",
    "alt.atheism",
    "comp.graphics",
    "comp.windows.x",
    "talk.politics.mideast",
    "talk.politics.guns",
]  # We'll only consider these categories out of 20 categories for speed.

newsgroups_train = fetch_20newsgroups(
    subset="train", remove=("headers", "footers", "quotes"), categories=cats
)
X_news_train, y_news_train = newsgroups_train.data, newsgroups_train.target
df = pd.DataFrame(X_news_train, columns=["text"])
df["target"] = y_news_train
df["target_name"] = [
    newsgroups_train.target_names[target] for target in newsgroups_train.target
]
df


newsgroups_train.target_names








import spacy
nlp = spacy.load("en_core_web_md", disable=["parser", "ner"])





import re

def preprocess(text):
    # Regex to remove URLs, emails, digits and line breaks
    text = re.sub(r'http\S+|www\S+', 'URL', text)
    text = re.sub(r'\S+@\S+', 'EMAIL', text)
#    text = re.sub(r'\d+', 'NUM', text)
    text = re.sub(r'[\\\/\n\r]+', ' ', text)

    doc = nlp(text)
    clean_text = []

    for token in doc:
        if not token.is_stop and not token.is_punct and not token.like_url and not token.like_email and not token.like_num:
            lemma = token.lemma_.lower().strip()
            clean_text.append(lemma)
            
    return " ".join(clean_text)


df["text_pp"] = df["text"].apply(preprocess)


display(df)


df.iloc[2:6]


























from sklearn.decomposition import LatentDirichletAllocation
import mglearn


cv = CountVectorizer()
doc_term_matrix = cv.fit_transform(df["text_pp"])


n_topics = 5
lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)
lda.fit(doc_term_matrix)

sorting = np.argsort(lda.components_, axis=1)[:, ::-1]
feature_names = np.array(cv.get_feature_names_out())

mglearn.tools.print_topics(
    topics=range(n_topics),
    feature_names=feature_names,
    sorting=sorting,
    topics_per_chunk=5,
    n_words=10,
)


n_topics = 8
lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)
lda.fit(doc_term_matrix)

sorting = np.argsort(lda.components_, axis=1)[:, ::-1]
feature_names = np.array(cv.get_feature_names_out())

mglearn.tools.print_topics(
    topics=range(n_topics),
    feature_names=feature_names,
    sorting=sorting,
    topics_per_chunk=4,
    n_words=10,
)


n_topics = 2
lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)
lda.fit(doc_term_matrix)

sorting = np.argsort(lda.components_, axis=1)[:, ::-1]
feature_names = np.array(cv.get_feature_names_out())

mglearn.tools.print_topics(
    topics=range(n_topics),
    feature_names=feature_names,
    sorting=sorting,
    topics_per_chunk=2,
    n_words=10,
)














n_topics = 5
lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)
lda.fit(doc_term_matrix)

sorting = np.argsort(lda.components_, axis=1)[:, ::-1]
feature_names = np.array(cv.get_feature_names_out())

mglearn.tools.print_topics(
    topics=range(n_topics),
    feature_names=feature_names,
    sorting=sorting,
    topics_per_chunk=5,
    n_words=10,
)











topic_dist = lda.transform(doc_term_matrix)


topics = ["Politics/Conflict", "General Discussion", "Sports/games", "Software/Computing", "Reglion/Belief"]

for i, topic in enumerate(topic_dist[:5]):
    print(f"\nDocument {i}: {topic}")
    print(f"Topic: {topics[np.argmax(topic)]}\n")
    print(df["text"].iloc[i])





















